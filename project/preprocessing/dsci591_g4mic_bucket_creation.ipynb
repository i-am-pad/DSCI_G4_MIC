{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from functools import reduce\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "import numpy as np\n",
    "from operator import mul\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "# only need to turn this on if you're intending to update the notebook\n",
    "# to copy samples to another new bucket / location for conversion to\n",
    "# images & saving to another images prefix.\n",
    "DO_COPY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>modify_time_utc</th>\n",
       "      <th>path</th>\n",
       "      <th>size_kilobytes</th>\n",
       "      <th>size_megabytes</th>\n",
       "      <th>dataset</th>\n",
       "      <th>kind</th>\n",
       "      <th>dest_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>396936</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>396.936</td>\n",
       "      <td>0.396936</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250800</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>250.800</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246975</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>246.975</td>\n",
       "      <td>0.246975</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367605</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>367.605</td>\n",
       "      <td>0.367605</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32782</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/03...</td>\n",
       "      <td>32.782</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_bytes       modify_time_utc  \\\n",
       "0      396936  2022-04-30T14:28:00Z   \n",
       "1      250800  2022-04-30T14:28:00Z   \n",
       "2      246975  2022-04-30T14:28:00Z   \n",
       "3      367605  2022-04-30T14:28:00Z   \n",
       "4       32782  2022-04-30T14:28:00Z   \n",
       "\n",
       "                                                path  size_kilobytes  \\\n",
       "0  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         396.936   \n",
       "1  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         250.800   \n",
       "2  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         246.975   \n",
       "3  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         367.605   \n",
       "4  gs://drexel_dsci_2022_g4mic/Benign/unzipped/03...          32.782   \n",
       "\n",
       "   size_megabytes dataset    kind dest_path  \n",
       "0        0.396936     pdf  benign            \n",
       "1        0.250800     pdf  benign            \n",
       "2        0.246975     pdf  benign            \n",
       "3        0.367605     pdf  benign            \n",
       "4        0.032782     pdf  benign            "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (pd.read_csv('file_sizes_analysis_data/sorel_pdf_dataset.csv')\n",
    "      .assign(dest_path = '')\n",
    "     )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24604 entries, 0 to 24603\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   size_bytes       24604 non-null  int64  \n",
      " 1   modify_time_utc  24604 non-null  object \n",
      " 2   path             24604 non-null  object \n",
      " 3   size_kilobytes   24604 non-null  float64\n",
      " 4   size_megabytes   24604 non-null  float64\n",
      " 5   dataset          24604 non-null  object \n",
      " 6   kind             24604 non-null  object \n",
      " 7   dest_path        24604 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dest_bucket = 'dsci591_g4mic'\n",
    "dest_bucket_benign_prefix = 'raw/benign/'\n",
    "dest_bucket_malicious_prefix = 'raw/malicious/'\n",
    "dest_bucket_benign_path = os.path.join('gs://', dest_bucket, dest_bucket_benign_prefix)\n",
    "dest_bucket_malware_path = os.path.join('gs://', dest_bucket, dest_bucket_malicious_prefix)\n",
    "\n",
    "s_file_names = df.path.str.split('/').apply(lambda x: x[-1])\n",
    "benign_mask = df.kind == 'benign'\n",
    "df.loc[benign_mask, 'dest_path'] = dest_bucket_benign_path + s_file_names[benign_mask]\n",
    "df.loc[~benign_mask, 'dest_path'] = dest_bucket_malware_path + s_file_names[~benign_mask]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Data Copying\n",
    "\n",
    "Split up because these are in two different projects at the moment. After fixing up perms on a DSCI591-G4 project service account so that it can manage objects in both projects, this became redundant. It could've just been one section for both file sets, since the src/dst paths are all built for both datasets. It separates info & permission sanity check for the origin datasets at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_copy(src_bucket, dst_bucket, data, verbose=True, exit_on_failure=True):\n",
    "    '''helper to facilitate copying data from one bucket to another using paths provided in dataset'''\n",
    "    to_copy = (data\n",
    "               .loc[:, ['path', 'dest_path']]\n",
    "               # just grabbing the prefix part of what are otherwise gs://bucket/prefix.\n",
    "               # using the full gs:// path is good w/ the tensorflow API, since it works\n",
    "               # with the VFS, and is quick in a Colab env. but it's slow locally.\n",
    "               .apply(lambda df_x: (df_x\n",
    "                                   .str.split('/', 3)\n",
    "                                   .apply(lambda r_x: r_x[-1])\n",
    "                                   )\n",
    "                      )\n",
    "               )\n",
    "    skipped = 0\n",
    "    copy_failures = 0\n",
    "    for _, rec in tqdm(list(to_copy.iterrows())):\n",
    "        dst = rec.dest_path\n",
    "        src = rec.path\n",
    "        \n",
    "        try:\n",
    "            from_blob = src_bucket.blob(src)\n",
    "            _ = src_bucket.copy_blob(from_blob, dst_bucket, dst)\n",
    "        except Exception as ex:\n",
    "            if exit_on_failure:\n",
    "                print(ex)\n",
    "                return\n",
    "            copy_failures += 1\n",
    "        \n",
    "        # too bad this is horribly slow compared to just using the storage API directly!\n",
    "        #tf.io.gfile.copy(src, dst, overwrite=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'completed copying {data.shape[0] - copy_failures - skipped}/{data.shape[0]} files with {skipped} already exists and {copy_failures} failures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nunique dataset (should be 1): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>malicious</th>\n",
       "      <td>13066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "kind            \n",
       "malicious  13066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>modify_time_utc</th>\n",
       "      <th>path</th>\n",
       "      <th>size_kilobytes</th>\n",
       "      <th>size_megabytes</th>\n",
       "      <th>dataset</th>\n",
       "      <th>kind</th>\n",
       "      <th>dest_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>681788</td>\n",
       "      <td>2022-05-04T21:42:21Z</td>\n",
       "      <td>gs://dsci591_g4_sorel20m/binaries_resampled/00...</td>\n",
       "      <td>681.788</td>\n",
       "      <td>0.681788</td>\n",
       "      <td>sorel</td>\n",
       "      <td>malicious</td>\n",
       "      <td>gs://dsci591_g4mic/raw/malicious/000024a1bdbd0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>116261</td>\n",
       "      <td>2022-05-04T21:40:22Z</td>\n",
       "      <td>gs://dsci591_g4_sorel20m/binaries_resampled/00...</td>\n",
       "      <td>116.261</td>\n",
       "      <td>0.116261</td>\n",
       "      <td>sorel</td>\n",
       "      <td>malicious</td>\n",
       "      <td>gs://dsci591_g4mic/raw/malicious/000148b941e11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>106209</td>\n",
       "      <td>2022-05-04T21:38:53Z</td>\n",
       "      <td>gs://dsci591_g4_sorel20m/binaries_resampled/00...</td>\n",
       "      <td>106.209</td>\n",
       "      <td>0.106209</td>\n",
       "      <td>sorel</td>\n",
       "      <td>malicious</td>\n",
       "      <td>gs://dsci591_g4mic/raw/malicious/0001ee2b649f6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>23503</td>\n",
       "      <td>2022-05-04T21:24:19Z</td>\n",
       "      <td>gs://dsci591_g4_sorel20m/binaries_resampled/00...</td>\n",
       "      <td>23.503</td>\n",
       "      <td>0.023503</td>\n",
       "      <td>sorel</td>\n",
       "      <td>malicious</td>\n",
       "      <td>gs://dsci591_g4mic/raw/malicious/0009398f92bb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11542</th>\n",
       "      <td>245423</td>\n",
       "      <td>2022-05-04T21:13:38Z</td>\n",
       "      <td>gs://dsci591_g4_sorel20m/binaries_resampled/00...</td>\n",
       "      <td>245.423</td>\n",
       "      <td>0.245423</td>\n",
       "      <td>sorel</td>\n",
       "      <td>malicious</td>\n",
       "      <td>gs://dsci591_g4mic/raw/malicious/0010e4169bb95...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       size_bytes       modify_time_utc  \\\n",
       "11538      681788  2022-05-04T21:42:21Z   \n",
       "11539      116261  2022-05-04T21:40:22Z   \n",
       "11540      106209  2022-05-04T21:38:53Z   \n",
       "11541       23503  2022-05-04T21:24:19Z   \n",
       "11542      245423  2022-05-04T21:13:38Z   \n",
       "\n",
       "                                                    path  size_kilobytes  \\\n",
       "11538  gs://dsci591_g4_sorel20m/binaries_resampled/00...         681.788   \n",
       "11539  gs://dsci591_g4_sorel20m/binaries_resampled/00...         116.261   \n",
       "11540  gs://dsci591_g4_sorel20m/binaries_resampled/00...         106.209   \n",
       "11541  gs://dsci591_g4_sorel20m/binaries_resampled/00...          23.503   \n",
       "11542  gs://dsci591_g4_sorel20m/binaries_resampled/00...         245.423   \n",
       "\n",
       "       size_megabytes dataset       kind  \\\n",
       "11538        0.681788   sorel  malicious   \n",
       "11539        0.116261   sorel  malicious   \n",
       "11540        0.106209   sorel  malicious   \n",
       "11541        0.023503   sorel  malicious   \n",
       "11542        0.245423   sorel  malicious   \n",
       "\n",
       "                                               dest_path  \n",
       "11538  gs://dsci591_g4mic/raw/malicious/000024a1bdbd0...  \n",
       "11539  gs://dsci591_g4mic/raw/malicious/000148b941e11...  \n",
       "11540  gs://dsci591_g4mic/raw/malicious/0001ee2b649f6...  \n",
       "11541  gs://dsci591_g4mic/raw/malicious/0009398f92bb3...  \n",
       "11542  gs://dsci591_g4mic/raw/malicious/0010e4169bb95...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sorel = df.loc[lambda df_x: df_x.dataset == 'sorel']\n",
    "print(f'nunique dataset (should be 1): {df_sorel.dataset.nunique()}')\n",
    "display(df_sorel.groupby('kind')['kind'].agg(['count']))\n",
    "display(df_sorel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials, project = google.auth.load_credentials_from_file(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "client = storage.Client(project=project)\n",
    "src_bucket = client.bucket('dsci591_g4_sorel20m')\n",
    "dst_bucket = client.bucket(dest_bucket)\n",
    "\n",
    "# sanity check!\n",
    "tf.io.gfile.exists(df_sorel.iloc[0].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13066/13066 [18:50<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed copying 13066/13066 files with 0 already exists and 0 failures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DO_COPY:\n",
    "    do_copy(src_bucket, dst_bucket, df_sorel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nunique dataset (should be 1): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>benign</th>\n",
       "      <td>8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malicious</th>\n",
       "      <td>3177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "kind            \n",
       "benign      8361\n",
       "malicious   3177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>modify_time_utc</th>\n",
       "      <th>path</th>\n",
       "      <th>size_kilobytes</th>\n",
       "      <th>size_megabytes</th>\n",
       "      <th>dataset</th>\n",
       "      <th>kind</th>\n",
       "      <th>dest_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>396936</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>396.936</td>\n",
       "      <td>0.396936</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td>gs://dsci591_g4mic/raw/benign/02eounrel.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250800</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>250.800</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td>gs://dsci591_g4mic/raw/benign/02frrltr.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246975</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>246.975</td>\n",
       "      <td>0.246975</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td>gs://dsci591_g4mic/raw/benign/02govbnd.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367605</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...</td>\n",
       "      <td>367.605</td>\n",
       "      <td>0.367605</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td>gs://dsci591_g4mic/raw/benign/02solp.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32782</td>\n",
       "      <td>2022-04-30T14:28:00Z</td>\n",
       "      <td>gs://drexel_dsci_2022_g4mic/Benign/unzipped/03...</td>\n",
       "      <td>32.782</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>pdf</td>\n",
       "      <td>benign</td>\n",
       "      <td>gs://dsci591_g4mic/raw/benign/030.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_bytes       modify_time_utc  \\\n",
       "0      396936  2022-04-30T14:28:00Z   \n",
       "1      250800  2022-04-30T14:28:00Z   \n",
       "2      246975  2022-04-30T14:28:00Z   \n",
       "3      367605  2022-04-30T14:28:00Z   \n",
       "4       32782  2022-04-30T14:28:00Z   \n",
       "\n",
       "                                                path  size_kilobytes  \\\n",
       "0  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         396.936   \n",
       "1  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         250.800   \n",
       "2  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         246.975   \n",
       "3  gs://drexel_dsci_2022_g4mic/Benign/unzipped/02...         367.605   \n",
       "4  gs://drexel_dsci_2022_g4mic/Benign/unzipped/03...          32.782   \n",
       "\n",
       "   size_megabytes dataset    kind                                    dest_path  \n",
       "0        0.396936     pdf  benign  gs://dsci591_g4mic/raw/benign/02eounrel.pdf  \n",
       "1        0.250800     pdf  benign   gs://dsci591_g4mic/raw/benign/02frrltr.pdf  \n",
       "2        0.246975     pdf  benign   gs://dsci591_g4mic/raw/benign/02govbnd.pdf  \n",
       "3        0.367605     pdf  benign     gs://dsci591_g4mic/raw/benign/02solp.pdf  \n",
       "4        0.032782     pdf  benign        gs://dsci591_g4mic/raw/benign/030.pdf  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_bucket = client.bucket('drexel_dsci_2022_g4mic')\n",
    "df_pdf = df.loc[lambda df_x: df_x.dataset == 'pdf']\n",
    "print(f'nunique dataset (should be 1): {df_sorel.dataset.nunique()}')\n",
    "display(df_pdf.groupby('kind')['kind'].agg(['count']))\n",
    "display(df_pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check!\n",
    "tf.io.gfile.exists(df_pdf.iloc[0].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11538/11538 [37:29<00:00,  5.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed copying 11510/11538 files with 0 already exists and 28 failures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DO_COPY:\n",
    "    do_copy(src_bucket, dst_bucket, df_pdf, exit_on_failure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: str,\n",
    "              dtype: str = 'uint8',\n",
    "              ) -> np.array:\n",
    "    '''reads a file from GCS into a numpy array\n",
    "    \n",
    "    args\n",
    "        path: GCS path to file in gs://bucket/prefix/filename format\n",
    "        dtype: dtype to shape data to\n",
    "    \n",
    "    returns\n",
    "        numpy array containing file's content represented as :param:`dtype` bytes\n",
    "    '''\n",
    "    fin = BytesIO(file_io.read_file_to_string(path, binary_mode=True))\n",
    "    return np.frombuffer(fin.read(), dtype='uint8')\n",
    "\n",
    "def write_file(data: np.array,\n",
    "               path: str,\n",
    "               ext: str = '.npy',\n",
    "               ):\n",
    "    '''writes a numpy array of the specified dtype to a file in a bucket on GCS\n",
    "    \n",
    "    args\n",
    "        data: numpy array containing bytes to write to GCS\n",
    "        path: path on GCS to save the file to\n",
    "        ext: extension to use for saved files. default is numpy's canonical .npy.\n",
    "    '''\n",
    "    pre, _ = os.path.splitext(path)\n",
    "    np.save(file_io.FileIO(pre + ext, 'w'), data)\n",
    "\n",
    "def convert_to_image(data: np.array,\n",
    "                     dimension: int,\n",
    "                     ):\n",
    "    '''implements file conversion from binary to square bytes used for grayscale image representation\n",
    "    \n",
    "    args:\n",
    "        data: a numpy array containing a file's contents with dtype=uint8\n",
    "        dimension: value used for H and W\n",
    "    \n",
    "    returns\n",
    "        a numpy 2D numpy array containing the truncated or zero-padded bytes of shape (dimension, dimension)\n",
    "    '''\n",
    "    target_shape = (dimension, dimension, 1)\n",
    "    total_bytes_allowed = reduce(mul, target_shape)\n",
    "    image = np.zeros(shape=(total_bytes_allowed,))\n",
    "    num_bytes = min(data.shape[0], total_bytes_allowed)\n",
    "    image[:num_bytes] = data[:num_bytes]\n",
    "    \n",
    "    return image\n",
    "\n",
    "def convert_to_images(dataset_meta: pd.DataFrame,\n",
    "                      dimension: int,\n",
    "                      src_prefix: str = '/raw/',\n",
    "                      dst_prefix: str = '/images/',\n",
    "                      ):\n",
    "    '''converts images in a Google Cloud Storage bucket to their reprensentation as an image. save format at\n",
    "    destination is a numpy array (*.npy).\n",
    "    \n",
    "    n.b. an extremely helpful ref used to help produce all of this: https://stackoverflow.com/questions/41633748/load-numpy-array-in-google-cloud-ml-job\n",
    "    \n",
    "    args\n",
    "        dataset_meta: dataframe containing the sorel_pdf_dataset.csv data from which dest_path will be used\n",
    "        dimension: value used for H and W\n",
    "        src_prefix: prefix value containing raw data; should be a substring data.dest_path\n",
    "        dst_prefix: prefix value where converted image data will be saved\n",
    "    '''\n",
    "    for _, rec in tqdm(list(dataset_meta.iterrows())):\n",
    "        # counterintuitive naming: rec.dest_path is with respect to what the sorel_pdf_dataset.csv specified\n",
    "        # when the data was originally copied form rec.path -> rec.dest_path to consolidate sorel + pdf datasets\n",
    "        # to one bucket for a common image conversion to be applied\n",
    "        src_path = rec.dest_path\n",
    "        data = read_file(src_path)\n",
    "        image_data = convert_to_image(data, dimension)\n",
    "        \n",
    "        dst_path = src_path.replace(src_prefix, dst_prefix)\n",
    "        write_file(data, dst_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 172/24604 [02:21<5:00:59,  1.35it/s]"
     ]
    }
   ],
   "source": [
    "# kind of slow but works well! it's because this is using the same stuff that the tf.io.GFile API is using\n",
    "convert_to_images(df, 648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36b7864b62b9ba22b6149af8f6ffbba92419c9ee7ee61ab59ef4e0d1f3b68df1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
